{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This api requestor was altered from\n",
    "https://github.com/ronentk/sci-paper-miner/blob/master/core_requestor.py,\n",
    "which was in turn adapted from\n",
    "https://github.com/oacore/or2016-api-demo/blob/master/OR2016%20API%20demo.ipynb\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CoreApiRequestor:\n",
    "\n",
    "    def __init__(self, endpoint, api_key):\n",
    "        self.endpoint = endpoint\n",
    "        self.api_key = api_key\n",
    "        #defaults\n",
    "        self.pagesize = 100\n",
    "        self.page = 1\n",
    "\n",
    "    def parse_response(self, decoded):\n",
    "        res = []\n",
    "        for item in decoded['data']:\n",
    "            doi = None\n",
    "            if 'identifiers' in item:\n",
    "                for identifier in item['identifiers']:\n",
    "                    if identifier and identifier.startswith('doi:'):\n",
    "                        doi = identifier\n",
    "                        break\n",
    "            res.append([item['title'], doi])\n",
    "        return res\n",
    "\n",
    "    def request_url(self, url):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html = response.read()\n",
    "        return html\n",
    "\n",
    "    def get_method_query_request_url(self,method,query,fullText,page):\n",
    "        if (fullText):\n",
    "            fullText = 'true'\n",
    "        else:\n",
    "            fullText = 'false'\n",
    "        params = {\n",
    "            'apiKey':self.api_key,\n",
    "            'page':page,\n",
    "            'pageSize':self.pagesize,\n",
    "            'fulltext':fullText\n",
    "        }\n",
    "        return self.endpoint + method + '/' + urllib.parse.quote(query) + '?' + urllib.parse.urlencode(params)\n",
    "\n",
    "    def get_up_to_20_pages_of_query(self,method,query,fulltext):\n",
    "        url = self.get_method_query_request_url(method,query,fulltext,1)\n",
    "        all_articles=[]\n",
    "        resp = self.request_url(url)\n",
    "        result = json.loads(resp.decode('utf-8'))\n",
    "        all_articles.append(result)\n",
    "#         if (result['totalHits']>100):\n",
    "#             numOfPages = int(result['totalHits']/self.pagesize)  #rounds down\n",
    "#             if (numOfPages>20):\n",
    "#                 numOfPages=20\n",
    "#             for i in range(2,numOfPages):\n",
    "#                 url = self.get_method_query_request_url(method,query,False,i)\n",
    "#                 print(url)\n",
    "#                 resp =self.request_url(url)\n",
    "#                 all_articles.append(json.loads(resp.decode('utf-8')))\n",
    "        return all_articles\n",
    "\n",
    "'''\n",
    "Example invokation\n",
    "'''\n",
    "# init \n",
    "endpoint = 'https://core.ac.uk/api-v2'\n",
    "\n",
    "'''\n",
    "********************************************\n",
    "Add your own api key below\n",
    "'''\n",
    "api_key = '9FwDugmWHIsPLbijSoY4OyBGaAEdeXnz'\n",
    "'''\n",
    "********************************************\n",
    "'''\n",
    "method = '/articles/search'\n",
    "api = CoreApiRequestor(endpoint,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = Path(\"./data/two-topic-fulltext.pkl\")\n",
    "\n",
    "if my_file.is_file():\n",
    "    pass\n",
    "else:\n",
    "    topics = ['deep AND learning', 'computer AND vision']\n",
    "\n",
    "    topic_results = [[], []]\n",
    "    for i, topic in enumerate(topics):\n",
    "        for page in range(1, 101):\n",
    "            '''\n",
    "            Get url\n",
    "            '''\n",
    "            url = api.get_method_query_request_url(method,topic,True,page)\n",
    "            url\n",
    "\n",
    "            '''\n",
    "            Get results\n",
    "            '''\n",
    "            response = api.request_url(url)\n",
    "            result = json.loads(response.decode('utf-8'))\n",
    "            topic_results[i].append(result[\"data\"])\n",
    "\n",
    "    d = []\n",
    "    for i, topic in enumerate(topics):\n",
    "        for page in topic_results[i]:\n",
    "            for article in page:\n",
    "                d.append({'topic': i, 'full_text': article['fullText']})\n",
    "    len(d)\n",
    "\n",
    "    # pickle dataframe for future use\n",
    "    df.to_pickle('./data/two-topic-fulltext.pkl')\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    first_article = full_articles[0][\"data\"][0]\n",
    "    pp.pprint(first_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
